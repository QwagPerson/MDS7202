{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto 1 - MDS7202 Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos üìö**\n",
    "\n",
    "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
    "\n",
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesor: Ignacio Meza, Gabriel Iturra\n",
    "- Auxiliar: Sebasti√°n Tinoco\n",
    "- Ayudante: Arturo Lazcano, Angelo Mu√±oz\n",
    "\n",
    "*Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir.*\n",
    "\n",
    "### Equipo:\n",
    "\n",
    "- Gustavo Santelices\n",
    "\n",
    "\n",
    "### Link de repositorio de GitHub: `https://github.com/QwagPerson/MDS7202`\n",
    "\n",
    "Fecha l√≠mite de entrega üìÜ: 27 de Octubre de 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Reglas\n",
    "\n",
    "- **Grupos de 2 personas.**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Estrictamente prohibida la copia. \n",
    "- Pueden usar cualquier material del curso que estimen conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://worldskateamerica.org/wp-content/uploads/2023/07/SANTIAGO-2023-1-768x153.jpg\" alt=\"Descripci√≥n de la imagen\">\n",
    "</div>\n",
    "\n",
    "En un Chile azotado por un profundo caos pol√≠tico-econ√≥mico y el resurgimiento de programas de televisi√≥n de dudosa calidad, todas las miradas y esperanzas son depositadas en el √©xito de un √∫nico evento: Santiago 2023. La naci√≥n necesitaba desesperadamente un respiro, y los Juegos de Santiago 2023 promet√≠an ser una luz al final del t√∫nel.\n",
    "\n",
    "El Presidente de la Rep√∫blica -conocido en las calles como Bomb√≠n-, consciente de la importancia de este evento para la revitalizaci√≥n del pa√≠s, decide convocar a usted y su equipo en calidad de expertos en an√°lisis de datos y estad√≠sticas. Con gran solemnidad, el presidente les encomienda una importante y peligrosa: liderar un proyecto que permitiera caracterizar de forma autom√°tica y eficiente los datos generados por estos magnos juegos. Para esto, el presidente le destaca que la soluci√≥n debe considerar los siguientes puntos:\n",
    "- Caracterizaci√≥n autom√°tica de los datos\n",
    "- La soluci√≥n debe ser compatible con cualquier dataset\n",
    "- Se les facilita el dataset *olimpiadas.parquet*, el cual recopila data de diferentes juegos ol√≠mpicos realizados en los √∫ltimos a√±os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Creaci√≥n de `Profiler` Class (4.0 puntos)\n",
    "\n",
    "Cree la clase `Profiler`. Como m√≠nimo, esta debe tener las siguientes funcionalidades:\n",
    "\n",
    "1. El m√©todo constructor, el cual debe recibir los datos a procesar en formato `Pandas DataFrame`. Adem√°s, este m√©todo debe generar una carpeta en su directorio de trabajo con el nombre `EDA_fecha`, donde `fecha` corresponda a la fecha de ejecuci√≥n en formato `DD-MM-YYYY`.\n",
    "\n",
    "2. El m√©todo `summarize`, el cual debe caracterizar las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Reportar el tipo de variable\n",
    "    - Reportar el n√∫mero y/o porcentaje de valores √∫nicos de la variable\n",
    "    - Reportar el n√∫mero y/o porcentaje de valores nulos\n",
    "    - Si la variables es num√©rica:\n",
    "        - Reportar el n√∫mero y/o porcentaje de valores cero, negativos y outliers\n",
    "        - Reportar estad√≠stica descriptiva como el valor m√≠nimo, m√°ximo, promedio y los percentiles 25, 50, 75 y 100\n",
    "   - Levantar una alerta en caso de encontrar alguna anomal√≠a fuera de lo com√∫n (el criterio debe ser ajustable por el usuario)\n",
    "   - Guardar sus resultados en el directorio `EDA_fecha/summary.txt`. El archivo debe separar de forma clara y ordenada los resultados de cada punto.\n",
    "\n",
    "3. El m√©todo `plot_vars`, el cual debe graficar la distribuci√≥n e interraciones de las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/plots`\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Para las variables num√©ricas:\n",
    "        - Genere un gr√°fico de distribuci√≥n de densidad\n",
    "        - Grafique la correlaci√≥n entre las variables\n",
    "    - Para las variables categ√≥ricas:\n",
    "        - Genere un histograma de las top N categor√≠as (N debe ser un par√°metro ajustable)\n",
    "        - Grafique el coeficiente V de Cramer entre las variables\n",
    "    - Guardar cada gr√°fico generado en la carpeta `EDA_fecha/plots` en formato `.pdf` y bajo el naming `variable.pdf`, donde `variable` es el nombre de la variable de inter√©s\n",
    "    \n",
    "4. El m√©todo `clean_data`, el cual debe limpiar los datos para que luego puedan ser procesados. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/clean_data`\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Drop de valores duplicados\n",
    "    - Implementar como m√≠nimo 2 t√©cnicas para tratar los valores nulos, como:\n",
    "        - Drop de valores nulos\n",
    "        - Imputar valores nulos con alguna t√©cnica de imputaci√≥n\n",
    "        - Funcionalidad para escoger entre una t√©cnica y la otra.\n",
    "    - Una de las columnas del dataframe presenta datos *no at√≥micos*. Separe dicha columna en las columnas que la compongan.\n",
    "        - Hint: ¬øQu√© caracteres permiten separar una columna de otra?\n",
    "        - Para las pruebas con el dataset nuevo, puede esperar que exista al menos una columna con este tipo de problema. Asuma que los separadores ser√°n los mismos, aunque el n√∫mero de columnas a separar puede ser distinto.\n",
    "    - Deber√≠an usar `FunctionTransformer`.\n",
    "    - Guardar los datos procesados en formato `.csv` en el path `EDA_fecha/clean_data/data.csv`\n",
    "\n",
    "5. El m√©todo `scale`, el cual debe preparar adecuadamente los datos para luego ser consumidos por alg√∫n tipo de algoritmo. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/scale`\n",
    "    - Procesar de forma adecuada los datos num√©ricos y categ√≥ricos:\n",
    "        - Su m√©todo debe recibir las t√©cnicas de escalamiento como argumento de entrada (utilizar solo t√©cnicas compatibles con el framework de `sklearn`)\n",
    "        - Para los atributos num√©ricos, se transforme los datos con un escalador logar√≠tmico y un `MinMaxScaler`\n",
    "        - Asuma que no existen datos ordinales en su dataset\n",
    "    - Guardar todo este procesamiento en un `ColumnTransformer`.\n",
    "    - Guardar los datos limpios y transformados en formato `.csv` en el path `EDA_fecha/process/scaled_features.csv`\n",
    "\n",
    "6. El m√©todo `make_clusters`, el cual debe generar clusters de los datos usando alg√∫n algoritmo de clusterizaci√≥n. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/clusters`\n",
    "    - Generar un estudio del codo donde se√±ale la cantidad de clusters optimos para el desarrollo.\n",
    "    - Su m√©todo debe recibir el algoritmo de clustering como argumento de entrada (utilizar solo algoritmos compatibles con el framework de `sklearn`).\n",
    "    - No olvide pre procesar adecuadamente los datos antes de implementar la t√©cnica de clustering. \n",
    "    - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "    - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "    - Una vez generado los clusters, proyecte los datos a 2 dimensiones usando su t√©cnica de reducci√≥n de dimensionalidad favorita y grafique los resultados coloreando por cluster.\n",
    "    - Guardar los datos con su respectivo cluster en formato `.csv` en el path `EDA_fecha/clusters/data_clusters.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "7. El m√©todo `detect_anomalies`, el cual debe detectar anomal√≠as en los datos. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "\n",
    "    - Crear la carpeta `EDA_fecha/anomalies`\n",
    "    - Implementar alguna t√©cnica de detecci√≥n de anomal√≠as.\n",
    "    - Al igual que el punto anterior, su m√©todo debe considerar los siguientes puntos:\n",
    "        - No olvide pre procesar de forma adecuada los datos antes de implementar la t√©cnica de detecci√≥n de anomal√≠a. \n",
    "        - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "        - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "        - Su m√©todo debe recibir el algoritmo como argumento de entrada\n",
    "        - Una vez generado las etiquetas, proyecte los datos a 2 dimensiones y grafique los resultados coloreando por las etiquetas predichas por el detector de anomal√≠as\n",
    "    - Guardar los datos con su respectiva etiqueta en formato `.csv` en el path `EDA_fecha/anomalies/data_anomalies.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "8. El m√©todo `profile`, el cual debe ejecutar todos los m√©todos anteriores.\n",
    "\n",
    "9. Crear el m√©todo `clearGarbage` para eliminar las carpetas/archivos creados/as por la clase `Profiler`.\n",
    "\n",
    "Algunas consideraciones generales:\n",
    "- Su clase ser√° testeada con datos tabulares diferentes a los provistos. No desarrollen c√≥digo *hardcodeado*: su clase debe ser capaz de funcionar para **cualquier** dataset. \n",
    "- Aplique todo su conocimiento sobre buenas pr√°cticas de programaci√≥n: se evaluar√° que su c√≥digo sea limpio y ordenado.\n",
    "- Recuerden documentar cada una de las funcionalidades que implementen.\n",
    "- Recuerden adjuntar sus `requirements.txt` junto a su entrega de proyecto. **El c√≥digo que no se pueda ejecutar por imcompatibilidades de librer√≠as no ser√° corregido.**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype, is_string_dtype\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from typing import Callable\n",
    "import warnings\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "import re\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "from typing_extensions import Self\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from typing import Type\n",
    "from typing import Union\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import chi2_contingency\n",
    "import shutil\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:02.238737851Z",
     "start_time": "2023-11-11T02:58:01.380017181Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Funciones auxiliares"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_or_create_folder(folder_path: str):\n",
    "    \"\"\"\n",
    "    Simple aux function to create a folder if it doesnt exists.\n",
    "    Always return the path passed.\n",
    "    :param folder_path: path to the folder\n",
    "    :return: folder_path:\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "    return folder_path\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:02.238997767Z",
     "start_time": "2023-11-11T02:58:01.853085986Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def cramers_v(var1,var2) :\n",
    "    \"\"\"\n",
    "    Calculates the cramer's V between 2 categoricals variables.\n",
    "    :param var1: Categorical variable\n",
    "    :param var2: Categorical variable\n",
    "    :return: cramer's V coef\n",
    "    \"\"\"\n",
    "    crosstab =np.array(pd.crosstab(var1,var2, rownames=None, colnames=None))\n",
    "    stat = chi2_contingency(crosstab)[0]\n",
    "    obs = np.sum(crosstab)\n",
    "    mini = min(crosstab.shape)-1\n",
    "    return np.sqrt(stat/(obs*mini))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:02.239316466Z",
     "start_time": "2023-11-11T02:58:01.867495042Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_cramer_matrix(input_data):\n",
    "    \"\"\"\n",
    "    Calculates the matrix of cramer's V coef between all\n",
    "    the categorical variables of input_data\n",
    "    :param input_data: an array with categorical variables \n",
    "    :return: an array matrix that is the result of calculating cramer's V\n",
    "    \"\"\"\n",
    "    cramer_matrix = np.zeros((input_data.shape[1], input_data.shape[1]))\n",
    "    for i in range(input_data.shape[1]):\n",
    "        for j in range(input_data.shape[1]):\n",
    "            cramer_matrix[i, j] = cramers_v(input_data[:, i], input_data[:, j])\n",
    "    return cramer_matrix\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:02.261165922Z",
     "start_time": "2023-11-11T02:58:01.881479756Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def log_transform(x):\n",
    "    return np.log(x + 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:02.328175329Z",
     "start_time": "2023-11-11T02:58:01.891192424Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variable objects"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el profiler muchas veces queremos tener comportamiento segun el tipo de variable que estamos viendo. De esto se encarga estas clases. Todas implementar la interfaz definida en AbstractVariable y pueden tener algunos metodos particulares. Son muy utiles y extensibles en caso que queramos trabajar con otras variables como imagenes o audios por ejemplo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class AbstractVariable(ABC):\n",
    "    \"\"\"\n",
    "    Base class for variables. \n",
    "    \"\"\"\n",
    "    def __init__(self, variable_data : pd.Series):\n",
    "        \"\"\"\n",
    "        Simple constructor that only takes data variable.\n",
    "        :param variable_data: \n",
    "        \"\"\"\n",
    "        self.variable_data = variable_data\n",
    "        \n",
    "    @abstractmethod\n",
    "    def summarize(self, param_dict : dict) -> str:\n",
    "        \"\"\"\n",
    "        Returns a summary of the variable.\n",
    "        :param param_dict: Dictionary used to contain parameters of the summary\n",
    "        :return: A string containing a commmon summary that every variable can use. \n",
    "        \"\"\"\n",
    "        summary = \"\"\n",
    "        summary += f\"\\tDtype: {self.variable_data.dtype}\\n\"\n",
    "        summary += f\"\\tLength: {len(self.variable_data)}\\n\"\n",
    "        summary += f\"\\tUnique values: {round(self.variable_data.nunique() / len(self.variable_data)*100,2)}% [{self.variable_data.nunique()}]\\n\"\n",
    "        summary += f\"\\tNull values: {round(self.variable_data.isna().sum() / len(self.variable_data)*100,2)}% [{self.variable_data.isna().sum()}]\\n\"\n",
    "        return summary\n",
    "    \n",
    "    @abstractmethod\n",
    "    def plot(self, ax: plt.Axes, param_dict : dict) -> plt.Axes:\n",
    "        \"\"\"\n",
    "        Method to plot the variable\n",
    "        :param ax: matplotlib axes where to plot the variable\n",
    "        :param param_dict: dictionary containing all necessary parameters to plot the variable.\n",
    "        :return: the ax passed as input with the new plot in it.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def filter_list(variables_list : List[Self]) -> List[Self]:\n",
    "        \"\"\"\n",
    "        Convenience static method used to filter all the variables of a type\n",
    "        from a list of variables.\n",
    "        :param variables_list: a list of variables\n",
    "        :return: variables of the type from this method was called\n",
    "        \"\"\"\n",
    "        pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:02.328826188Z",
     "start_time": "2023-11-11T02:58:01.945055194Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class TextVariable(AbstractVariable):\n",
    "    \"\"\"\n",
    "    Text variables are composed of strings with no order between them.\n",
    "    For example a list of name could be considered a text variable or the description\n",
    "    of an item from a store dataset.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def filter_list(variables_list: List[AbstractVariable]) -> List[AbstractVariable]:\n",
    "        \"\"\"\n",
    "        Convenience static method used to filter all the variables of a type\n",
    "        from a list of variables.\n",
    "        :param variables_list: a list of variables\n",
    "        :return: A list of Text variables\n",
    "        \"\"\"\n",
    "        return [x for x in variables_list if isinstance(x, TextVariable)]\n",
    "\n",
    "    def summarize(self, param_dict : dict) -> str:\n",
    "        common_summary = super().summarize(param_dict)\n",
    "        summary = \"\"\n",
    "        summary += f\"Report of text variable: {self.variable_data.name}\\n\"\n",
    "        summary += common_summary\n",
    "        return summary\n",
    "\n",
    "    def plot(self, ax: plt.Axes, param_dict : dict) -> plt.Axes:\n",
    "        return self.variable_data.value_counts().sort_values(ascending=False).head(param_dict[\"n_categories\"]).plot(ax=ax, kind=\"bar\")\n",
    "    \n",
    "    \n",
    "    def split_text_column(self, separators : List[str]) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Splits the text of a column using the separators in separators\n",
    "        Note: it drops the NaN!.\n",
    "        :param separators: the characters that are used to separate the text in the variable data.\n",
    "        :return: a series where every element is the result of splitting using the separators\n",
    "        \"\"\"\n",
    "        sep_regex = \"|\".join([f'[{sep}]+' for sep in separators])\n",
    "        split_series = self.variable_data.dropna().astype(\"string\").apply(lambda x : re.split(sep_regex,x))\n",
    "        return split_series\n",
    "    \n",
    "    def check_if_not_atomic(self, separators : List[str]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if all elements of the variable data can be separated into the same\n",
    "        amount of elements to try to detect automatically if a variable is not atomic. \n",
    "        :param separators: the characters that are used to separate the text in the variable data.\n",
    "        :return: True if not atomic False in every other case.\n",
    "        \"\"\"\n",
    "        length_of_lists = self.split_text_column(separators).apply(len)\n",
    "        return length_of_lists.nunique() == 1 and (length_of_lists > 1).all()\n",
    "    \n",
    "    def extract_not_atomic_data(self, separators : List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract a dataframe containing the result of splitting a variable using the separators.\n",
    "        :param separators: the characters that are used to separate the text in the variable data.\n",
    "        :return: A dataframe containing the columns that result of splitting. \n",
    "        \"\"\"\n",
    "        if not self.check_if_not_atomic(separators):\n",
    "            raise ValueError(\"Trying to extract data from atomic data.\")\n",
    "        \n",
    "        split_series = self.split_text_column(separators)\n",
    "        max_length = split_series.apply(len).max()\n",
    "        result_series = []\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            partial_result = split_series.apply(lambda x : x[i] if i < len(x) else np.nan)\n",
    "            partial_result.name=f\"{self.variable_data.name}_splitted_{i}\"\n",
    "            result_series.append(partial_result)\n",
    "            \n",
    "        return pd.concat(result_series, axis=1)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:02.329193815Z",
     "start_time": "2023-11-11T02:58:01.945525105Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class NumericVariable(AbstractVariable):\n",
    "    @staticmethod\n",
    "    def filter_list(variables_list: List[AbstractVariable]) -> List[AbstractVariable]:\n",
    "        return [x for x in variables_list if isinstance(x, NumericVariable)]\n",
    "    \n",
    "    def get_out_interquartile_range(self, scale_factor: float = 1.5):\n",
    "        \"\"\"\n",
    "        Obtains the iqr and returns the data that is outside of the iqr.\n",
    "        :param scale_factor: Scaling factor to measure if a variable is outside the iqr.\n",
    "        :return: Data outside of the iqr.\n",
    "        \"\"\"\n",
    "        desc = self.variable_data.describe()\n",
    "        iqr = desc[\"75%\"] - desc[\"25%\"]\n",
    "        cota_inf = desc[\"25%\"] - iqr * scale_factor\n",
    "        cota_sup = desc[\"75%\"] + iqr * scale_factor\n",
    "        return self.variable_data.loc[(self.variable_data < cota_inf) | (cota_sup < self.variable_data)]\n",
    "    \n",
    "    def process_anomalies(self, anomaly_criterion : Pipeline = None):\n",
    "        \"\"\"\n",
    "        Uses the pipe of anomaly criterion to mark anomalies.\n",
    "        Notice! Anomaly criterion should run as an univariate pipeline taking the data\n",
    "        and returning the mask of anomalies. \n",
    "        :param anomaly_criterion: A pipeline that can run on a series of numeric data\n",
    "        :return: Boolean mask of anomalies found.\n",
    "        \"\"\"\n",
    "        anomalies = anomaly_criterion.fit_transform(self.variable_data)\n",
    "        if anomalies.sum() != 0:\n",
    "            warnings.warn(f\"Warning: For variable of name {self.variable_data.name} there was {anomalies.sum()} found!\")\n",
    "        return anomalies\n",
    "    \n",
    "    \n",
    "    def summarize(self, param_dict : dict) -> str:\n",
    "            common_summary = super().summarize(param_dict)\n",
    "            series_descriptor = self.variable_data.describe()\n",
    "            out_of_iqr_anomalies = self.get_out_interquartile_range()\n",
    "            anomaly_criterion = param_dict.get(\"anomaly_criterion\") \\\n",
    "                if \"anomaly_criterion\" in param_dict.keys() else None\n",
    "            \n",
    "            summary = \"\"\n",
    "            summary += f\"Report of Numeric variable: {self.variable_data.name}\\n\"\n",
    "            summary += common_summary\n",
    "            summary += f\"\\tZero values: {round((self.variable_data==0).sum() / len(self.variable_data)*100,2)}% [{(self.variable_data==0).sum()}]\\n\"\n",
    "            summary += f\"\\tNegative values: {round((self.variable_data<0).sum() / len(self.variable_data)*100,2)}% [{(self.variable_data<0).sum()}]\\n\"\n",
    "            summary += f\"\\tAnomalies amount: {round(len(out_of_iqr_anomalies) / len(self.variable_data)*100,2)}% [{len(out_of_iqr_anomalies)}]\\n\"\n",
    "            summary += f\"\\tMin: {series_descriptor['min']} \\tMax: {series_descriptor['max']} \\tMean: {series_descriptor['mean']}\\n\"\n",
    "            summary += f\"\\t25%: {series_descriptor['25%']} \\t50%: {series_descriptor['50%']} \\t75%: {series_descriptor['75%']} \\t100%:{series_descriptor['max']}\\n\"\n",
    "            \n",
    "            if anomaly_criterion is not None:\n",
    "                anomalies = self.process_anomalies(anomaly_criterion)\n",
    "                summary += f\"\\tAnomalies detected using custom criterion: {anomalies.sum()} \\n\"\n",
    "            return summary\n",
    "    \n",
    "    def plot(self, ax: plt.Axes, param_dict : dict) -> plt.Axes:\n",
    "        return self.variable_data.plot(ax=ax, kind=\"kde\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:02.329556759Z",
     "start_time": "2023-11-11T02:58:01.994402651Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class CategoricalVariable(AbstractVariable):\n",
    "    @staticmethod\n",
    "    def filter_list(variables_list: List[AbstractVariable]) -> List[AbstractVariable]:\n",
    "        return [x for x in variables_list if isinstance(x, CategoricalVariable)]\n",
    "\n",
    "    def plot(self, ax: plt.Axes, param_dict : dict) -> plt.Axes:\n",
    "        return self.variable_data.value_counts().sort_values(ascending=False).head(param_dict[\"n_categories\"]).plot(ax=ax, kind=\"bar\")\n",
    "\n",
    "    def summarize(self, param_dict : dict) -> str:\n",
    "        common_summary = super().summarize(param_dict)\n",
    "        summary = \"\"\n",
    "        summary += f\"Report of Categorical variable: {self.variable_data.name}\\n\"\n",
    "        summary += common_summary\n",
    "        summary += (f\"\\tFirst {param_dict['first_n_categorical']} categories:\"\n",
    "                    f\" {self.variable_data.value_counts()[0:param_dict['first_n_categorical']].index.tolist()}\\n\")\n",
    "        return summary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:02.329942160Z",
     "start_time": "2023-11-11T02:58:01.994624392Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class ObjectVariable(AbstractVariable):\n",
    "    @staticmethod\n",
    "    def filter_list(variables_list: List[AbstractVariable]) -> List[AbstractVariable]:\n",
    "        return [x for x in variables_list if isinstance(x, ObjectVariable)]\n",
    "    \n",
    "    def summarize(self, param_dict : dict) -> str:\n",
    "        common_summary = super().summarize(param_dict)\n",
    "        summary = \"\"\n",
    "        summary += f\"Report of Object variable: {self.variable_data.name}\\n\"\n",
    "        summary += common_summary\n",
    "        return summary\n",
    "\n",
    "    def plot(self, ax: plt.Axes, param_dict : dict) -> plt.Axes:\n",
    "        return self.variable_data.value_counts().sort_values(ascending=False).head(param_dict[\"n_categories\"]).plot(ax=ax, kind=\"bar\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:02.330214395Z",
     "start_time": "2023-11-11T02:58:01.994805048Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### VariablesBuilder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class VariablesHandler:\n",
    "    \"\"\"\n",
    "    Utility class that makes easier the interface of variables.\n",
    "    It allows to build variables from a dataframe, infer the variable type from a \n",
    "    series and obtain the dataframe from a list of variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determines the fraction of data that a text variable can have has unique categories\n",
    "    # for it to be considered a categorical variable instead of a text variable.\n",
    "    PERCENTAGE_OF_DATA_AS_CATEGORIES = 0.01 \n",
    "    @staticmethod\n",
    "    def infer_variable_type(variable_data : np.array) -> Type[AbstractVariable]:\n",
    "        \"\"\"\n",
    "        Analyses the data of a variable and returns the class it belongs to. \n",
    "        :param variable_data: data of a variable\n",
    "        :return: The class of variable that the data hints it is.\n",
    "        \"\"\"\n",
    "        is_str = np.vectorize(lambda i: isinstance(i, str) or i is None)\n",
    "        is_numeric = np.vectorize(lambda i: isinstance(i, int) or isinstance(i, float) or i is None)\n",
    "        \n",
    "        variables_temp_series = pd.Series(variable_data)\n",
    "        uniques = len(variables_temp_series.dropna().unique())\n",
    "        length = len(variable_data)\n",
    "                \n",
    "        if is_str(variable_data).all() and uniques < int(VariablesHandler.PERCENTAGE_OF_DATA_AS_CATEGORIES * length):\n",
    "            return CategoricalVariable\n",
    "        elif is_str(variable_data).all():\n",
    "            return TextVariable\n",
    "        elif is_numeric(variable_data).all():\n",
    "            return NumericVariable\n",
    "        else:\n",
    "            return ObjectVariable\n",
    "            \n",
    "    @staticmethod\n",
    "    def build_variable(variable_data : np.array, variable_name : str) -> AbstractVariable:\n",
    "        \"\"\"\n",
    "        Builds a variable object from variable data and variable name\n",
    "        :param variable_data: data of a variable\n",
    "        :param variable_name: name of the variable\n",
    "        :return: The variable object of that variable\n",
    "        \"\"\"\n",
    "        variable_series = pd.Series(data=variable_data, name=variable_name).infer_objects()\n",
    "        variable_type_class = VariablesHandler.infer_variable_type(variable_data)\n",
    "        return variable_type_class(variable_series)\n",
    "\n",
    "    @staticmethod\n",
    "    def bulk_build_variables(variables_data : np.array, variables_names: List[str]) -> List[AbstractVariable]:\n",
    "        \"\"\"\n",
    "        Builds variables in a bulk from a list\n",
    "        :param variables_data: \n",
    "        :param variables_names: \n",
    "        :return: A list of variables\n",
    "        \"\"\"\n",
    "        if not variables_data.shape[1] == len(variables_names):\n",
    "            raise ValueError(f\"Lists variables_data and variables_names should be the same length got\"\n",
    "                             f\" {len(variables_data)} != {len(variables_names)}\")\n",
    "        \n",
    "        output_variables = []\n",
    "        \n",
    "        for idx in range(variables_data.shape[1]):\n",
    "            var_data = variables_data[:, idx]\n",
    "            var_name = variables_names[idx]\n",
    "            output_variables.append(VariablesHandler.build_variable(var_data, var_name))\n",
    "            \n",
    "        return output_variables \n",
    "    \n",
    "    @staticmethod\n",
    "    def collect_data_from_variables(variables : List[AbstractVariable]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transforms a list of variables into a dataframe using the name and data of those\n",
    "        variables.\n",
    "        :param variables: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        variables_data = [x.variable_data for x in variables]\n",
    "        lengths = [len(x) for x in variables_data]\n",
    "        \n",
    "        if len(lengths) == 0:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        if not len(set(lengths))==1:\n",
    "            raise Exception(f\"All variables from input should have the same length.\"\n",
    "                            f\"Got lengths {set(lengths)}\")\n",
    "        \n",
    "        return pd.concat(variables_data, axis=1)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:02.330435219Z",
     "start_time": "2023-11-11T02:58:02.036667241Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summarizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class Summarizer:\n",
    "    \"\"\"\n",
    "    Builds and save the summary from a list of variables    \n",
    "    \"\"\"\n",
    "    def __init__(self, variables : List[AbstractVariable]):\n",
    "        self.variables = variables\n",
    "    \n",
    "    def get_summary(self, param_dict : dict) -> str:\n",
    "        \"\"\"\n",
    "        Obtains the summary of a list of variables\n",
    "        :param param_dict: \n",
    "        :return: string containing the summary\n",
    "        \"\"\"\n",
    "        summary = \"\"\n",
    "        for variable in self.variables:\n",
    "            summary += variable.summarize(param_dict)\n",
    "            summary += \"---\\n\"\n",
    "        return summary\n",
    "    \n",
    "    def save_summary(self, path : str, param_dict : dict) -> None:\n",
    "        \"\"\"\n",
    "        Saves the summary to path\n",
    "        :param path: \n",
    "        :param param_dict: \n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        with open(path, \"w\") as output_file:\n",
    "            output_file.write(self.get_summary(param_dict))\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:02.330609622Z",
     "start_time": "2023-11-11T02:58:02.036867986Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class Profiler:\n",
    "    \"\"\"\n",
    "    Main class to study the behaviour of a class.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_data : pd.DataFrame,\n",
    "            handle_null_pipeline : Pipeline,\n",
    "            separators : List[str],\n",
    "            handle_null_extracted_features_pipeline : Pipeline,\n",
    "            numeric_data_scaling_transform : Pipeline,\n",
    "            categorical_data_scaling_transform : Pipeline,\n",
    "            \n",
    "    ):\n",
    "        \"\"\"\n",
    "        Builds the profiler and creates the folder where its results are stored.\n",
    "        Notice: Note that text variables are discarded in scale so there isn't a \"text_data_scaling_transform\"\n",
    "        :param input_data: data to analyze\n",
    "        :param handle_null_pipeline:  pipeline to handle nulls of input_data\n",
    "        :param separators:  Separators to split text vars to check if they are not atomic\n",
    "        :param handle_null_extracted_features_pipeline: pipeline to handle nulls of extracted features from non atomic features\n",
    "        :param numeric_data_scaling_transform: pipeline to scale numerics variables \n",
    "        :param categorical_data_scaling_transform:  pipeline to scale categorical variables\n",
    "        \"\"\"\n",
    "        self.input_data = input_data\n",
    "        self.cleaned_data = None\n",
    "        self.scaled_data = None\n",
    "\n",
    "        self.handle_null_pipeline = handle_null_pipeline.set_output(transform=\"pandas\")\n",
    "        self.handle_null_extracted_features_pipeline = handle_null_extracted_features_pipeline.set_output(transform=\"pandas\")\n",
    "        self.numeric_data_scaling_transform = numeric_data_scaling_transform.set_output(transform=\"pandas\")\n",
    "        self.categorical_data_scaling_transform = categorical_data_scaling_transform.set_output(transform=\"pandas\")\n",
    "\n",
    "        self.root_path = get_or_create_folder(f\"EDA_{dt.date.today().strftime('%d-%m-%Y')}\")\n",
    "        self.separators = separators\n",
    "\n",
    "    @staticmethod\n",
    "    def get_or_set_variable_names(amount_of_variables, variable_names):\n",
    "        \"\"\"\n",
    "        Creates automatic names for variables if they dont have any.\n",
    "        :param amount_of_variables: \n",
    "        :param variable_names: \n",
    "        :return: a list of names for variables\n",
    "        \"\"\"\n",
    "        if variable_names is None:\n",
    "            variable_names=[f\"variable_{i}\" for i in range(amount_of_variables)]\n",
    "        return variable_names\n",
    "        \n",
    "        \n",
    "    def summarize(\n",
    "            self,\n",
    "            input_data : np.array,\n",
    "            variables_names : List[str] = None,\n",
    "            param_dict = None\n",
    "    )-> Self:\n",
    "        \"\"\"\n",
    "        Summarizes the data of the variables passed as input\n",
    "        :param input_data: data of the variables\n",
    "        :param variables_names: names of the variables\n",
    "        :param param_dict: param dict to be passed to the summarizer, here we can control\n",
    "        some parameters of the summary if needed.\n",
    "        :return: Self for method chaining.\n",
    "        \"\"\"\n",
    "        if param_dict is None: \n",
    "            param_dict = {\n",
    "                \"first_n_categorical\" : 10,\n",
    "            }\n",
    "            \n",
    "        variables_names = Profiler.get_or_set_variable_names(input_data.shape[1], variables_names)\n",
    "        \n",
    "        summarizer = Summarizer(\n",
    "            variables = VariablesHandler.bulk_build_variables(input_data, variables_names)\n",
    "        )\n",
    "\n",
    "        output_file_path = os.path.join(\n",
    "            self.root_path,\n",
    "            \"summary.txt\"\n",
    "        )\n",
    "        \n",
    "        summarizer.save_summary(path=output_file_path, param_dict=param_dict)\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_corr(ax : plt.Axes, input_data : np.array, variable_names : List[str] = None) -> plt.Axes:\n",
    "        \"\"\"\n",
    "        Plot the correlation matrix in ax from data in input_data with names of variable names\n",
    "        :param ax: \n",
    "        :param input_data: \n",
    "        :param variable_names: \n",
    "        :return: ax containing the new plot.\n",
    "        \"\"\"\n",
    "        variable_names= Profiler.get_or_set_variable_names(input_data.shape[1], variable_names)\n",
    "        corr_matrix = pd.DataFrame(input_data, columns=variable_names).corr()\n",
    "        out = sns.heatmap(corr_matrix, annot=True, ax=ax)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_cramers_v(ax : plt.Axes, input_data : np.array, variable_names : List[str] = None) -> plt.Axes:\n",
    "        \"\"\"\n",
    "        Plot the cramer's V matrix in ax from data in input_data with names of variable names\n",
    "        :param ax: \n",
    "        :param input_data: \n",
    "        :param variable_names: \n",
    "        :return: ax containing the new plot.\n",
    "        \"\"\"\n",
    "        variable_names= Profiler.get_or_set_variable_names(input_data.shape[1], variable_names)\n",
    "        cramer_matrix = pd.DataFrame(get_cramer_matrix(input_data), columns=variable_names, index=variable_names)\n",
    "        out=sns.heatmap(cramer_matrix, annot=True, ax=ax)\n",
    "        return out\n",
    "    \n",
    "    def plot_vars(\n",
    "            self,\n",
    "            input_data : np.array,\n",
    "            variable_names : List[str] = None,\n",
    "            plotting_params : dict = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots the var passed in input. Plotting params is a dictionary that can be passed to the variables.\n",
    "        This function stills needs refactoring as it does too many things at once.\n",
    "        :param input_data: \n",
    "        :param variable_names: \n",
    "        :param plotting_params: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        if plotting_params is None:\n",
    "            plotting_params = {\n",
    "                'n_categories': 10\n",
    "            }\n",
    "\n",
    "        output_path = get_or_create_folder(os.path.join(self.root_path, \"plots\"))\n",
    "        variable_names= Profiler.get_or_set_variable_names(input_data.shape[1], variable_names)\n",
    "        \n",
    "        \n",
    "        variables = VariablesHandler.bulk_build_variables(input_data, variable_names)\n",
    "        \n",
    "        for var in variables:\n",
    "            fig, ax = plt.subplots()\n",
    "            var.plot(ax=ax, param_dict=plotting_params)\n",
    "            ax.get_figure().savefig(os.path.join(output_path, f\"{var.variable_data.name}.pdf\"), format='pdf', bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "\n",
    "        numerics = NumericVariable.filter_list(variables)\n",
    "        categoricals = CategoricalVariable.filter_list(variables)\n",
    "        \n",
    "        if len(numerics) > 0:\n",
    "            fig, ax = plt.subplots()\n",
    "            numerics_df = VariablesHandler.collect_data_from_variables(numerics)\n",
    "            Profiler.plot_corr(\n",
    "                ax=ax,\n",
    "                input_data=numerics_df.values,\n",
    "                variable_names=numerics_df.columns\n",
    "            )\n",
    "            ax.get_figure().savefig(os.path.join(output_path, f\"correlation_matrix.pdf\"), format='pdf')\n",
    "            plt.close(fig)\n",
    "            \n",
    "        if len(categoricals) > 0:\n",
    "            fig, ax = plt.subplots()\n",
    "            categoricals_df = VariablesHandler.collect_data_from_variables(categoricals)\n",
    "            Profiler.plot_cramers_v(\n",
    "                ax=ax,\n",
    "                input_data=categoricals_df.values,\n",
    "                variable_names=categoricals_df.columns\n",
    "            )\n",
    "            ax.get_figure().savefig(os.path.join(output_path, f\"cramers_v_matrix.pdf\"), format='pdf')\n",
    "            plt.close(fig)\n",
    "        \n",
    "    def clean_data(\n",
    "            self,\n",
    "            target_variables : List[str],\n",
    "            \n",
    "    ) -> Self:\n",
    "        \"\"\"\n",
    "        Cleans the data passed from target_variables using the defined pipelines.\n",
    "        Extracts data from non atomic columns if there is any.\n",
    "        :param target_variables: \n",
    "        :return: Self for method chaining.\n",
    "        \"\"\"\n",
    "        \n",
    "        output_path = get_or_create_folder(os.path.join(self.root_path, \"clean_data\"))\n",
    "        self.cleaned_data = self.input_data.copy()[target_variables]\n",
    "        self.cleaned_data = self.cleaned_data.drop_duplicates()\n",
    "        self.cleaned_data = self.handle_null_pipeline.fit_transform(self.cleaned_data)\n",
    "        \n",
    "        cleaned_data_var_names = self.cleaned_data.columns\n",
    "        cleaned_data_var_data = self.cleaned_data.values\n",
    "        \n",
    "        clean_variables = VariablesHandler.bulk_build_variables(\n",
    "            variables_data=cleaned_data_var_data,\n",
    "            variables_names=cleaned_data_var_names\n",
    "        )\n",
    "        \n",
    "        text_vars = TextVariable.filter_list(clean_variables)\n",
    "        non_atomic_text_vars = [x for x in text_vars if x.check_if_not_atomic(self.separators)]\n",
    "        \n",
    "        if len(non_atomic_text_vars) != 0:\n",
    "            extracted_features = pd.concat(\n",
    "                [x.extract_not_atomic_data(self.separators) for x in non_atomic_text_vars],\n",
    "                axis=1\n",
    "            )\n",
    "            extracted_features = self.handle_null_extracted_features_pipeline.fit_transform(extracted_features)\n",
    "\n",
    "            for x in non_atomic_text_vars:\n",
    "                clean_variables.remove(x)\n",
    "        \n",
    "            self.cleaned_data = pd.concat([\n",
    "                VariablesHandler.collect_data_from_variables(clean_variables),\n",
    "                extracted_features\n",
    "            ], axis = 1)\n",
    "            \n",
    "        else:\n",
    "            self.cleaned_data = VariablesHandler.collect_data_from_variables(clean_variables)\n",
    "        \n",
    "        self.cleaned_data.to_csv(\n",
    "            os.path.join(output_path, \"data.csv\"),\n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def scale(\n",
    "            self,\n",
    "    ) -> Self:\n",
    "        \"\"\"\n",
    "        Scales the data using the pipelines for categorical and numeric data.\n",
    "        Notice: It drops text data!\n",
    "        :return: Self for method chaining.\n",
    "        \"\"\"\n",
    "        output_path = get_or_create_folder(os.path.join(self.root_path, \"scaled\"))\n",
    "        if self.cleaned_data is None:\n",
    "            raise ValueError(\"Clean data should be defined before executing the scaling. Use clean_data method before using scale.\")\n",
    "        \n",
    "        variables = VariablesHandler.bulk_build_variables(self.cleaned_data.values, self.cleaned_data.columns)\n",
    "        numeric_vars = NumericVariable.filter_list(variables)\n",
    "        text_vars = TextVariable.filter_list(variables) # Dropped off text vars! ojito\n",
    "        categorical_vars = CategoricalVariable.filter_list(variables)\n",
    "        \n",
    "        numeric_data_df = VariablesHandler.collect_data_from_variables(numeric_vars)\n",
    "        categorical_data_df = VariablesHandler.collect_data_from_variables(categorical_vars)\n",
    "        \n",
    "        scaling_pipeline = Pipeline([\n",
    "            (\"scaling by var type\", ColumnTransformer([\n",
    "                (\"scale numerics vars\", self.numeric_data_scaling_transform, numeric_data_df.columns),\n",
    "                (\"scale categorical vars\", self.categorical_data_scaling_transform, categorical_data_df.columns)\n",
    "            ]))\n",
    "        ]).set_output(transform=\"pandas\")\n",
    "\n",
    "        df = pd.concat([numeric_data_df, categorical_data_df], axis=1)\n",
    "        self.scaled_data = scaling_pipeline.fit_transform(df)\n",
    "        self.scaled_data.to_csv(os.path.join(output_path, \"scaled_features.csv\"))\n",
    "        return self\n",
    "        \n",
    "    \n",
    "    def make_clusters(\n",
    "            self,\n",
    "            target_variables : List[str],\n",
    "            clustering_algorithm : Pipeline,\n",
    "            dim_reduction_algorithm : Pipeline,\n",
    "    ) -> Self:\n",
    "        \"\"\"\n",
    "        Forms clusters and saves it results using both clustering algorithm passed\n",
    "        and dim reduction for plotting.\n",
    "        :param target_variables: \n",
    "        :param clustering_algorithm: Pipeline with the clustering estimator as the final step.\n",
    "        :param dim_reduction_algorithm: Pipeline that reduced the dimension of data for plotting.\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        self.clean_data(target_variables).scale()\n",
    "        output_path = get_or_create_folder(os.path.join(self.root_path, \"clusters\"))\n",
    "        \n",
    "        clusters_labels = clustering_algorithm.fit_predict(self.scaled_data)\n",
    "        projected_data = dim_reduction_algorithm.fit_transform(self.scaled_data)\n",
    "        \n",
    "        # plot clusters\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(projected_data[:,0] , projected_data[:,1] , c = clusters_labels)\n",
    "        ax.get_figure().savefig(os.path.join(output_path, f\"clusters_pca.pdf\"), format='pdf')\n",
    "        plt.close(fig)\n",
    "        \n",
    "        output = self.scaled_data.copy()\n",
    "        output[\"cluster\"] = clusters_labels\n",
    "        output.to_csv(os.path.join(output_path, \"data_clusters.csv\"))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def detect_anomalies(\n",
    "            self,\n",
    "            target_variables : List[str],\n",
    "            anomaly_detection_algorithm : Pipeline,\n",
    "            dim_reduction_algorithm : Pipeline,\n",
    "    ) -> Self:\n",
    "        \"\"\"\n",
    "        detects anomalies and saves it results using both anomaly detection algorithm passed\n",
    "        and dim reduction for plotting.\n",
    "        :param target_variables: \n",
    "        :param anomaly_detection_algorithm: Pipeline with the anomaly detector estimator as the final step.\n",
    "        :param dim_reduction_algorithm: Pipeline that reduced the dimension of data for plotting.\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        self.clean_data(target_variables).scale()\n",
    "        output_path = get_or_create_folder(os.path.join(self.root_path, \"anomalies\"))\n",
    "\n",
    "        anomalies_labels = anomaly_detection_algorithm.fit_predict(self.scaled_data)\n",
    "        projected_data = dim_reduction_algorithm.fit_transform(self.scaled_data)\n",
    "\n",
    "        # plot anomalies\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(projected_data[:,0] , projected_data[:,1] , c = anomalies_labels)\n",
    "        ax.get_figure().savefig(os.path.join(output_path, f\"anomalies_pca.pdf\"), format='pdf')\n",
    "        plt.close(fig)\n",
    "\n",
    "        output = self.scaled_data.copy()\n",
    "        output[\"anomalies\"] = anomalies_labels\n",
    "        output.to_csv(os.path.join(output_path, \"data_anomalies.csv\"))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def profile(\n",
    "            self,\n",
    "            target_variables : List[str],\n",
    "            clustering_algorithm : Pipeline,\n",
    "            clustering_dim_reduction_algorithm : Pipeline,\n",
    "            anomaly_detection_algorithm : Pipeline,\n",
    "            anomaly_detection_dim_reduction_algorithm : Pipeline,\n",
    "            summarize_param_dict = None,\n",
    "            plot_param_dict = None\n",
    "    ) -> Self:\n",
    "        \"\"\"\n",
    "        Calls all methods of the profiler using the variables passed as input. \n",
    "        :param target_variables: Names of target variables should all be in self.input.columns\n",
    "        :param clustering_algorithm: See make clusters docs.\n",
    "        :param clustering_dim_reduction_algorithm: See make clusters docs.\n",
    "        :param anomaly_detection_algorithm: See make detect_anomalies docs.\n",
    "        :param anomaly_detection_dim_reduction_algorithm: See make detect_anomalies docs.\n",
    "        :param summarize_param_dict: Param dict passed to summarize method calls\n",
    "        :param plot_param_dict: Param dict passed to plot_param method calls\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        self.summarize(self.input_data[target_variables].values, target_variables, summarize_param_dict)\n",
    "        self.plot_vars(self.input_data[target_variables].values, target_variables, plot_param_dict)\n",
    "        self.clean_data(target_variables)\n",
    "        self.scale()\n",
    "        self.make_clusters(target_variables, clustering_algorithm, clustering_dim_reduction_algorithm)\n",
    "        self.detect_anomalies(target_variables, anomaly_detection_algorithm, anomaly_detection_dim_reduction_algorithm)\n",
    "        return self\n",
    "    \n",
    "    def clear_garbage(self) -> Self:\n",
    "        \"\"\"\n",
    "        Deletes the folder containing the EDA generated by the profiler.\n",
    "        :return: self for method chaining.\n",
    "        \"\"\"\n",
    "        shutil.rmtree(self.root_path)\n",
    "        return self\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:02.364914762Z",
     "start_time": "2023-11-11T02:58:02.039491837Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Caracterizar datos de Olimpiadas (2.0 puntos)\n",
    "\n",
    "A partir de la clase que hemos desarrollado previamente, procederemos a realizar un an√°lisis exhaustivo de los datos proporcionados en el enunciado. Este an√°lisis se presentar√° en forma de un informe contenido en el mismo Jupyter Notebook y abordar√° los siguientes puntos:\n",
    "\n",
    "1. Introducci√≥n\n",
    "    - Se proporcionar√° una breve descripci√≥n del problema que estamos abordando y se explicar√° la metodolog√≠a que se seguir√°.\n",
    "\n",
    "Elaborar una breve introducci√≥n con todo lo necesario para entender qu√© realizar√°n durante su proyecto. La idea es que describan de manera formal el proyecto con sus propias palabras y logren describir algunos aspectos b√°sicos tanto del dataset como del an√°lisis a realizar sobre los datos.\n",
    "\n",
    "Por lo anterior, en esta secci√≥n ustedes deber√°n ser capaces de:\n",
    "\n",
    "- Describir la tarea asociada al dataset.\n",
    "- Describir brevemente los datos de entrada que les provee el problema.\n",
    "- Plantear hip√≥tesis de c√≥mo podr√≠an abordar el problema.\n",
    "\n",
    "2. An√°lisis del EDA (An√°lisis Exploratorio de Datos)\n",
    "    - Se discutir√°n las observaciones y conclusiones obtenidas acerca de los datos proporcionados. A lo largo de su respuesta, debe responder preguntas como:\n",
    "        - ¬øComo se comportan las variables num√©ricas? ¬øy las categ√≥ricas?\n",
    "        - ¬øExisten valores nulos en el dataset? ¬øEn qu√© columnas? ¬øCuantos?\n",
    "        - ¬øCu√°les son las categor√≠as y frecuencias de las variables categ√≥ricas?\n",
    "        - ¬øExisten datos duplicados en el conjunto?\n",
    "        - ¬øExisten relaciones o patrones visuales entre las variables?\n",
    "        - ¬øExisten anomal√≠as notables o preocupantes en los datos?\n",
    "3. Creaci√≥n de Clusters y Anomal√≠as\n",
    "    - Se justificar√° la elecci√≥n de los algoritmos a utilizar y sus hiperpar√°metros. En el caso de clustering, justifique adem√°s el n√∫mero de clusters.\n",
    "    \n",
    "4. An√°lisis de Resultados\n",
    "    - Se examinar√°n los resultados obtenidos a partir de los cl√∫sters y anomal√≠as generadas. ¬øSe logra una separaci√≥n efectiva de los datos? Entregue una interpretaci√≥n de lo que representa cada cl√∫ster y anomal√≠a.\n",
    "5. Conclusi√≥n\n",
    "    - Se resumir√°n las principales conclusiones del an√°lisis y se destacar√°n las implicaciones pr√°cticas de los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informe"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Introducci√≥n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el presente informe se busca analizar de manera exhaustiva los datos de las olimpiadas de a√±os anteriores con el prop√≥sito de entender como se relacionan diferentes variables como el sexo, altura o peso con la obtenci√≥n de las medallas de las olimpiadas. Para eso se explorar√° un conjunto de datos de los deportistas ol√≠mpicos de los a√±os pasados y sus medallas obtenidas. Se comenzar√° realizando un an√°lisis exploratorio de datos donde se describir√°n los datos con lo que se cuentan, se identificaran correlaciones y patrones entre las variables. \n",
    "\n",
    "Luego se crear√°n grupos de deportistas para ver si se puede identificar alguna tendencia en los grupos e identificar variables que sean determinantes en la creaci√≥n de estos. Adem√°s, se identificar√°n si existen anomal√≠as en estos datos e intentar explicar el porque son consideradas como tal. Finalmente, se entregar√° un resumen de las conclusiones obtenidas de este an√°lisis y las posibles l√≠neas de investigaci√≥n."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### An√°lisis exploratorio de datos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "    ID                                Name Sex            Team  NOC  \\\n0    1                           A Dijiang   M           China  CHN   \n1    2                            A Lamusi   M           China  CHN   \n2    3                 Gunnar Nielsen Aaby   M         Denmark  DEN   \n3    4                Edgar Lindenau Aabye   M  Denmark/Sweden  DEN   \n4    5            Christine Jacoba Aaftink   F     Netherlands  NED   \n..  ..                                 ...  ..             ...  ...   \n95  32                Olav Augunson Aarnes   M          Norway  NOR   \n96  33                 Mika Lauri Aarnikka   M         Finland  FIN   \n97  33                 Mika Lauri Aarnikka   M         Finland  FIN   \n98  34  Jamale (Djamel-) Aarrass (Ahrass-)   M          France  FRA   \n99  35              Dagfinn Sverre Aarskog   M          Norway  NOR   \n\n          Games  Year  Season       City          Sport  \\\n0   1992 Summer  1992  Summer  Barcelona     Basketball   \n1   2012 Summer  2012  Summer     London           Judo   \n2   1920 Summer  1920  Summer  Antwerpen       Football   \n3   1900 Summer  1900  Summer      Paris     Tug-Of-War   \n4   1988 Winter  1988  Winter    Calgary  Speed Skating   \n..          ...   ...     ...        ...            ...   \n95  1912 Summer  1912  Summer  Stockholm      Athletics   \n96  1992 Summer  1992  Summer  Barcelona        Sailing   \n97  1996 Summer  1996  Summer    Atlanta        Sailing   \n98  2012 Summer  2012  Summer     London      Athletics   \n99  1998 Winter  1998  Winter     Nagano      Bobsleigh   \n\n                               Event Medal age-height-weight  \n0        Basketball Men's Basketball  None   24.0*180.0?80.0  \n1       Judo Men's Extra-Lightweight  None   23.0(170.0?60.0  \n2            Football Men's Football  None      24.0(nan?nan  \n3        Tug-Of-War Men's Tug-Of-War  Gold      34.0:nan?nan  \n4   Speed Skating Women's 500 metres  None   21.0(185.0?82.0  \n..                               ...   ...               ...  \n95         Athletics Men's High Jump  None      23.0:nan?nan  \n96   Sailing Men's Two Person Dinghy  None   24.0?187.0?76.0  \n97   Sailing Men's Two Person Dinghy  None   28.0?187.0?76.0  \n98      Athletics Men's 1,500 metres  None   30.0:187.0?76.0  \n99              Bobsleigh Men's Four  None   24.0:190.0?98.0  \n\n[100 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Team</th>\n      <th>NOC</th>\n      <th>Games</th>\n      <th>Year</th>\n      <th>Season</th>\n      <th>City</th>\n      <th>Sport</th>\n      <th>Event</th>\n      <th>Medal</th>\n      <th>age-height-weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>A Dijiang</td>\n      <td>M</td>\n      <td>China</td>\n      <td>CHN</td>\n      <td>1992 Summer</td>\n      <td>1992</td>\n      <td>Summer</td>\n      <td>Barcelona</td>\n      <td>Basketball</td>\n      <td>Basketball Men's Basketball</td>\n      <td>None</td>\n      <td>24.0*180.0?80.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>A Lamusi</td>\n      <td>M</td>\n      <td>China</td>\n      <td>CHN</td>\n      <td>2012 Summer</td>\n      <td>2012</td>\n      <td>Summer</td>\n      <td>London</td>\n      <td>Judo</td>\n      <td>Judo Men's Extra-Lightweight</td>\n      <td>None</td>\n      <td>23.0(170.0?60.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Gunnar Nielsen Aaby</td>\n      <td>M</td>\n      <td>Denmark</td>\n      <td>DEN</td>\n      <td>1920 Summer</td>\n      <td>1920</td>\n      <td>Summer</td>\n      <td>Antwerpen</td>\n      <td>Football</td>\n      <td>Football Men's Football</td>\n      <td>None</td>\n      <td>24.0(nan?nan</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Edgar Lindenau Aabye</td>\n      <td>M</td>\n      <td>Denmark/Sweden</td>\n      <td>DEN</td>\n      <td>1900 Summer</td>\n      <td>1900</td>\n      <td>Summer</td>\n      <td>Paris</td>\n      <td>Tug-Of-War</td>\n      <td>Tug-Of-War Men's Tug-Of-War</td>\n      <td>Gold</td>\n      <td>34.0:nan?nan</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Christine Jacoba Aaftink</td>\n      <td>F</td>\n      <td>Netherlands</td>\n      <td>NED</td>\n      <td>1988 Winter</td>\n      <td>1988</td>\n      <td>Winter</td>\n      <td>Calgary</td>\n      <td>Speed Skating</td>\n      <td>Speed Skating Women's 500 metres</td>\n      <td>None</td>\n      <td>21.0(185.0?82.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>32</td>\n      <td>Olav Augunson Aarnes</td>\n      <td>M</td>\n      <td>Norway</td>\n      <td>NOR</td>\n      <td>1912 Summer</td>\n      <td>1912</td>\n      <td>Summer</td>\n      <td>Stockholm</td>\n      <td>Athletics</td>\n      <td>Athletics Men's High Jump</td>\n      <td>None</td>\n      <td>23.0:nan?nan</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>33</td>\n      <td>Mika Lauri Aarnikka</td>\n      <td>M</td>\n      <td>Finland</td>\n      <td>FIN</td>\n      <td>1992 Summer</td>\n      <td>1992</td>\n      <td>Summer</td>\n      <td>Barcelona</td>\n      <td>Sailing</td>\n      <td>Sailing Men's Two Person Dinghy</td>\n      <td>None</td>\n      <td>24.0?187.0?76.0</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>33</td>\n      <td>Mika Lauri Aarnikka</td>\n      <td>M</td>\n      <td>Finland</td>\n      <td>FIN</td>\n      <td>1996 Summer</td>\n      <td>1996</td>\n      <td>Summer</td>\n      <td>Atlanta</td>\n      <td>Sailing</td>\n      <td>Sailing Men's Two Person Dinghy</td>\n      <td>None</td>\n      <td>28.0?187.0?76.0</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>34</td>\n      <td>Jamale (Djamel-) Aarrass (Ahrass-)</td>\n      <td>M</td>\n      <td>France</td>\n      <td>FRA</td>\n      <td>2012 Summer</td>\n      <td>2012</td>\n      <td>Summer</td>\n      <td>London</td>\n      <td>Athletics</td>\n      <td>Athletics Men's 1,500 metres</td>\n      <td>None</td>\n      <td>30.0:187.0?76.0</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>35</td>\n      <td>Dagfinn Sverre Aarskog</td>\n      <td>M</td>\n      <td>Norway</td>\n      <td>NOR</td>\n      <td>1998 Winter</td>\n      <td>1998</td>\n      <td>Winter</td>\n      <td>Nagano</td>\n      <td>Bobsleigh</td>\n      <td>Bobsleigh Men's Four</td>\n      <td>None</td>\n      <td>24.0:190.0?98.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows √ó 13 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_parquet(\"olimpiadas.parquet\")\n",
    "data.head(100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:02.707491612Z",
     "start_time": "2023-11-11T02:58:02.119650344Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se hara uso de la clase profiler para poder explorar y limpiar los datos entregados de manera rapida y comoda."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def fill_nan_medal(df_in):\n",
    "    df_in.Medal = df_in.Medal.fillna(\"No Medal\")\n",
    "    return df_in"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:02.707850856Z",
     "start_time": "2023-11-11T02:58:02.649843104Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo_santelices/Documents/Universidad/Primavera 2023/MDS7202/proyecto1/venv/lib/python3.8/site-packages/sklearn/preprocessing/_function_transformer.py:345: UserWarning: With transform=\"pandas\", `func` should return a DataFrame to follow the set_output API.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_profiler = Profiler(\n",
    "    input_data=data,\n",
    "    handle_null_pipeline=Pipeline([\n",
    "        ('Replacing values none in Medal column', FunctionTransformer(fill_nan_medal)),\n",
    "        ('Replacing with most common value', SimpleImputer(strategy=\"most_frequent\"))\n",
    "    ]),\n",
    "    separators=[\":\", \"(\", \"*\", \"?\", \"/\"],\n",
    "    handle_null_extracted_features_pipeline=Pipeline([\n",
    "        (\"Replacing nan values with mean\", SimpleImputer(strategy=\"mean\"))\n",
    "    ]),\n",
    "    numeric_data_scaling_transform=Pipeline([\n",
    "        (\"log transform\", FunctionTransformer(log_transform)),\n",
    "        (\"MinMax\", MinMaxScaler())\n",
    "    ]),\n",
    "    categorical_data_scaling_transform=Pipeline([\n",
    "        (\"One-Hot\", OneHotEncoder())\n",
    "    ])\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:02.708106035Z",
     "start_time": "2023-11-11T02:58:02.655566164Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el siguiente resumen podemos ver la informaci√≥n de las variables del dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report of Numeric variable: ID\n",
      "\tDtype: int64\n",
      "\tLength: 271116\n",
      "\tUnique values: 50.0% [135571]\n",
      "\tNull values: 0.0% [0]\n",
      "\tZero values: 0.0% [0]\n",
      "\tNegative values: 0.0% [0]\n",
      "\tAnomalies amount: 0.0% [0]\n",
      "\tMin: 1.0 \tMax: 135571.0 \tMean: 68248.95439590434\n",
      "\t25%: 34643.0 \t50%: 68205.0 \t75%: 102097.25 \t100%:135571.0\n",
      "---\n",
      "Report of text variable: Name\n",
      "\tDtype: object\n",
      "\tLength: 271116\n",
      "\tUnique values: 49.7% [134732]\n",
      "\tNull values: 0.0% [0]\n",
      "---\n",
      "Report of Categorical variable: Sex\n",
      "\tDtype: object\n",
      "\tLength: 271116\n",
      "\tUnique values: 0.0% [2]\n",
      "\tNull values: 0.0% [0]\n",
      "\tFirst 10 categories: ['M', 'F']\n",
      "---\n",
      "Report of Categorical variable: Team\n",
      "\tDtype: object\n",
      "\tLength: 271116\n",
      "\tUnique values: 0.44% [1184]\n",
      "\tNull values: 0.0% [0]\n",
      "\tFirst 10 categories: ['United States', 'France', 'Great Britain', 'Italy', 'Germany', 'Canada', 'Japan', 'Sweden', 'Australia', 'Hungary']\n",
      "---\n",
      "Report of Categorical variable: NOC\n",
      "\tDtype: object\n",
      "\tLength: 271116\n",
      "\tUnique values: 0.08% [230]\n",
      "\tNull values: 0.0% [0]\n",
      "\tFirst 10 categories: ['USA', 'FRA', 'GBR', 'ITA', 'GER', 'CAN', 'JPN', 'SWE', 'AUS', 'HUN']\n",
      "---\n",
      "Report of Categorical variable: Games\n",
      "\tDtype: object\n",
      "\tLength: 271116\n",
      "\tUnique values: 0.02% [51]\n",
      "\tNull values: 0.0% [0]\n",
      "\tFirst 10 categories: ['2000 Summer', '1996 Summer', '2016 Summer', '2008 Summer', '2004 Summer', '1992 Summer', '2012 Summer', '1988 Summer', '1972 Summer', '1984 Summer']\n",
      "---\n",
      "Report of Numeric variable: Year\n",
      "\tDtype: int64\n",
      "\tLength: 271116\n",
      "\tUnique values: 0.01% [35]\n",
      "\tNull values: 0.0% [0]\n",
      "\tZero values: 0.0% [0]\n",
      "\tNegative values: 0.0% [0]\n",
      "\tAnomalies amount: 0.14% [380]\n",
      "\tMin: 1896.0 \tMax: 2016.0 \tMean: 1978.3784800601957\n",
      "\t25%: 1960.0 \t50%: 1988.0 \t75%: 2002.0 \t100%:2016.0\n",
      "---\n",
      "Report of Categorical variable: Season\n",
      "\tDtype: object\n",
      "\tLength: 271116\n",
      "\tUnique values: 0.0% [2]\n",
      "\tNull values: 0.0% [0]\n",
      "\tFirst 10 categories: ['Summer', 'Winter']\n",
      "---\n",
      "Report of Categorical variable: City\n",
      "\tDtype: object\n",
      "\tLength: 271116\n",
      "\tUnique values: 0.02% [42]\n",
      "\tNull values: 0.0% [0]\n",
      "\tFirst 10 categories: ['London', 'Athina', 'Sydney', 'Atlanta', 'Rio de Janeiro', 'Beijing', 'Barcelona', 'Los Angeles', 'Seoul', 'Munich']\n",
      "---\n",
      "Report of Categorical variable: Sport\n",
      "\tDtype: object\n",
      "\tLength: 271116\n",
      "\tUnique values: 0.02% [66]\n",
      "\tNull values: 0.0% [0]\n",
      "\tFirst 10 categories: ['Athletics', 'Gymnastics', 'Swimming', 'Shooting', 'Cycling', 'Fencing', 'Rowing', 'Cross Country Skiing', 'Alpine Skiing', 'Wrestling']\n",
      "---\n",
      "Report of Categorical variable: Event\n",
      "\tDtype: object\n",
      "\tLength: 271116\n",
      "\tUnique values: 0.28% [765]\n",
      "\tNull values: 0.0% [0]\n",
      "\tFirst 10 categories: [\"Football Men's Football\", \"Ice Hockey Men's Ice Hockey\", \"Hockey Men's Hockey\", \"Water Polo Men's Water Polo\", \"Basketball Men's Basketball\", \"Cycling Men's Road Race, Individual\", \"Gymnastics Men's Individual All-Around\", \"Rowing Men's Coxed Eights\", \"Gymnastics Men's Team All-Around\", \"Handball Men's Handball\"]\n",
      "---\n",
      "Report of Categorical variable: Medal\n",
      "\tDtype: object\n",
      "\tLength: 271116\n",
      "\tUnique values: 0.0% [3]\n",
      "\tNull values: 85.33% [231333]\n",
      "\tFirst 10 categories: ['Gold', 'Bronze', 'Silver']\n",
      "---\n",
      "Report of text variable: age-height-weight\n",
      "\tDtype: object\n",
      "\tLength: 271116\n",
      "\tUnique values: 30.6% [82971]\n",
      "\tNull values: 0.0% [0]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "data_profiler.summarize(data.values, data.columns)\n",
    "with open(os.path.join(data_profiler.root_path, \"summary.txt\"), \"r\") as f:\n",
    "    print(f.read())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T02:58:07.880863273Z",
     "start_time": "2023-11-11T02:58:02.680274021Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se detecta que un 85% de los datos en Medal son nulos. Esto se debe a que el valor nulo representa que no hay medalla para ese deportista. Este dato igual entrega informaci√≥n por lo que no seria sensible desechar esas filas. No se detectan otros nulls.\n",
    "\n",
    "Se puede ver que existen varias columnas categoricas como Sex, Team o NOC. Y se encuentran solo dos variables numericas, ID y Year."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora observemos la distribuci√≥n de las variables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_profiler.plot_vars(\n",
    "    input_data=data.values,\n",
    "    variable_names=data.columns,\n",
    "    plotting_params = {\n",
    "        'n_categories': 20,\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-11T02:58:07.857103053Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adjunto se entregan las imagenes de los graficos, en esta secci√≥n comentaremos variable a variable si se observa algo relevante."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Age-height-weight: Se puede observar que existen varios deportistas sin informaci√≥n ya que hay varias columnas de la forma nanSnanSnan donde S es un valor separador. Habra que considerar esto en el pipeline de manejo de nulls de los datos extraidos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "City: Se puede ver que la ciudad donde m√°s veces se ha llevado a cabo los juegos olimpicos, para todo el resto de ciudad la distribuci√≥n es m√°s uniforme segun cuantas veces hayan llevado a cabo los juegos olimpicos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Event: Se puede observar que la distribuci√≥n es principalmente uniforme excepto con Football Men's teniendo mas deportistas que hayan participado en el."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Games: La distribuci√≥n es principalmente uniforme con una peque√±a tendencia creciente. Logicamente se tienen m√°s registros de los a√±os m√°s recientes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ID: Los ID's son repetidos 8 veces en promedio por cada ID, esto puede entregar una intuicion de en cuantos juegos olimpicos participa un deportista."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Medal: La visualizaci√≥n de los datos crudos solamente permite afirmar que existen cantidad casi iguales de medallas de oro, plata y bronce lo que es muy logico con respecto al conocimiento de como funcionan los juegos olimpicos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Name: Los nombres tambien son uniformes reptiendose una cantidad de 40 veces excepto qu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOC: Se tiene una distribuci√≥n exponencial donde el primer NOC es mucho m√°s comun que el segundo y asi sucesivamente."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Season: Se tiene mucha mayor cantidad de JJOO en Summer que en Winter. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sex: Es mucho mayor la participaci√≥n masculina que femenina."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sport: Tiene una distribuci√≥n exponencial."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Team: Al igual que Sport tiene una distribuci√≥n exponencial."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Year: Tiene mucha mayor densidad en los a√±os posteriores a 1950. Pero tambien observamos registros con a√±os anteriores a 1900 lo que es muy extra√±o ya que estos registros probablemente no sean signficativos para el analisis debido a su antiguedad."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_profiler.summarize(\n",
    "    data[['Year']].values,\n",
    "    ['Year'],\n",
    "    param_dict={'anomaly_criterion': Pipeline([(\"Year before 1900.\", FunctionTransformer(lambda x: x < 1900))])})\n",
    "\n",
    "with open(os.path.join(data_profiler.root_path, \"summary.txt\"), \"r\") as f:\n",
    "    print(f.read())"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos ver que son 380 registros anteriores a 1900! Esto es algo extra√±o."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora analizemos la matriz de correlacion y la matriz de cramer's V para ver las correlaciones entre variables numericas y categoricas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Al ver la matriz de correlaci√≥n se puede observar que no existen mayores correlaciones."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Al ver la matriz de cramer's V se puede ver que existe gran correlaci√≥n entre variables como Season y City o Event y Sex. Todas estas correlaciones tienen sentido pues ambas variables identifican parcialmente una a la otra. Por ejemplo, los juegos olimpicos al jugarse segregados por sexo se tiene la correlaci√≥n entre el evento y el sexo es muy alta."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente se analizara la cantidad de duplicados."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Existen un total de\", len(data) - len(data.drop_duplicates()), \"registros duplicados\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.loc[data.duplicated()]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Al analizar esta muestra podemos detectar con son todos registros anteriores al a√±o 1950 y de deportes Art Comptetitions y Sailing estos hechos es una posible causa de su duplicaci√≥n. Bastara con descartarlos del analisis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora hagamos un clustering de los datos y estudiemos las anomalias."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clustering y detecci√≥n de anomal√≠as"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se utilizaran solo las columnas que resultan logicas usar caracterizar la participacion de un deportista en los juegos olimpicos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_profiler = Profiler(\n",
    "    input_data=data,\n",
    "    handle_null_pipeline=Pipeline([\n",
    "        ('Replacing values none in Medal column', FunctionTransformer(fill_nan_medal)),\n",
    "        ('Replacing with most common value', SimpleImputer(strategy=\"most_frequent\"))\n",
    "    ]),\n",
    "    separators=[\":\", \"(\", \"*\", \"?\", \"/\"],\n",
    "    handle_null_extracted_features_pipeline=Pipeline([\n",
    "        (\"Replacing nan values with mean\", SimpleImputer(strategy=\"mean\"))\n",
    "    ]),\n",
    "    numeric_data_scaling_transform=Pipeline([\n",
    "        (\"log transform\", FunctionTransformer(log_transform)),\n",
    "        (\"MinMax\", MinMaxScaler())\n",
    "    ]),\n",
    "    categorical_data_scaling_transform=Pipeline([\n",
    "        # Using sparse false to ease pandas integration\n",
    "        (\"One-Hot\", OneHotEncoder(sparse_output=False))\n",
    "    ])\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "COLUMNAS_ELEGIDAS = [\n",
    "    'Sex',\n",
    "    'Sport',\n",
    "    'age-height-weight',\n",
    "    'Medal'\n",
    "]\n",
    "data_profiler.clean_data(COLUMNAS_ELEGIDAS).scale().scaled_data\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(Aca falto usar el metodo del codo para ver la cantidad de clusters pero no me dio tiempo :c tambien deberia haber usado K-modes en vez de Kmeans para manejar mejor las variables categoricas )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_profiler.make_clusters(\n",
    "    target_variables=COLUMNAS_ELEGIDAS,\n",
    "    clustering_algorithm=Pipeline([('K-means', KMeans(random_state=0, n_clusters=5))]),\n",
    "    dim_reduction_algorithm=Pipeline([('PCA', PCA(n_components=2))])\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se obtienen 5 clusters bien separables los que representan distintos tipos de deportistas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_profiler.detect_anomalies(\n",
    "    target_variables=COLUMNAS_ELEGIDAS,\n",
    "    anomaly_detection_algorithm=Pipeline([('Isoforest', IsolationForest(random_state=0))]),\n",
    "    dim_reduction_algorithm=Pipeline([('PCA', PCA(n_components=2))])\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(Dejo esto tambien como ejemplo de como utilizar la clase Profiler que cree)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_profiler.profile(\n",
    "    target_variables=COLUMNAS_ELEGIDAS,\n",
    "    clustering_algorithm=Pipeline([('K-means', KMeans(random_state=0, n_clusters=5))]),\n",
    "    clustering_dim_reduction_algorithm=Pipeline([('PCA', PCA(n_components=2))]),\n",
    "    anomaly_detection_algorithm=Pipeline([('Isoforest', IsolationForest(random_state=0))]),\n",
    "    anomaly_detection_dim_reduction_algorithm=Pipeline([('PCA', PCA(n_components=2))])\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analisis de Resultados"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se obtuvo un analisis de las variables de los juegos olimpicos, se identifico como se juegan mucho mas en verano, existen registros erroneos anteriores a 1950 y existe altisima correlaci√≥n entre las variables categoricas como sex y event."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusi√≥n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Falto m√°s tiempo para hacer este informe :(\n",
    "Al menos el codigo quedo piola,\n",
    "falto automatizar el metodo del codo pero decidi no hacerlo porque no le encontraba sentido ya que te forzaba a usar kmeans como algoritmo de clustering. Ahora como esta se puede usar cualquiera y deberia funcionar todo bien."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
